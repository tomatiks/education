{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"img/ml_theme.png\">\n",
    "# Вводный курс по нейронным сетям\n",
    "<center>\n",
    "**Автор материала: программист-исследователь Mail.Ru Group  Юрий Кашницкий**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Часть 4. Эффективное обучение нейронных сетей\n",
    "<center>Данный материал создан на базе [лекций](https://habrahabr.ru/company/mailru/blog/344982/) \"Техносферы\"\n",
    "</center> \n",
    "\n",
    "**План:**\n",
    "1. Инициализация весов\n",
    " - Xavier\n",
    " - He\n",
    "2. \"Умная\" регуляризация\n",
    " - Dropout\n",
    " - Batch Normalization\n",
    "3. Функции активации\n",
    "4. Продвинутые методы оптимизации\n",
    " - SGD + Momentum\n",
    " - Adagrad\n",
    " - RMSProp\n",
    " - Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Инициализация весов\n",
    "## Инициализация Xavier (Glorot)\n",
    "\n",
    "> *\"Understanding the difficulty of training deep feedforward neural networks\"* [X. Glorot, Y. Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) (University of Montreal, 2010)\n",
    "\n",
    "Для нечетной функции активации с единичной производной в нуле, например, `tanh`.\n",
    "\n",
    "<img src='img/tanh_and_sigmoid.png' width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Дисперсия активаций и градиентов (вывод у доски):\n",
    "\n",
    "$$\\large z^{i+1} = f(z^iW^i)$$\n",
    "$$\\large \\mathbb{D}[z^i] = \\mathbb{D}[x]\\prod_{k=0}^{i-1}n_k\\mathbb{D}[W^k]$$\n",
    "$$\\large \\mathbb{D}[\\frac{\\partial L}{\\partial s^i}] = \\mathbb{D}[\\frac{\\partial L}{\\partial s^d}] \\prod_{k=i}^{d}n_{k+1}\\mathbb{D}[W^k]$$\n",
    "Здесь $n_i$ – размерность $i$-го слоя.\n",
    "\n",
    "Хорошей инициализацией будет такая, что градиенты не взрываются и не затухают, то есть:\n",
    "\n",
    "$$\\forall i, j \\  \\mathbb{D}[z^i] = \\mathbb{D}[z^j],\\  \\mathbb{D}[\\frac{\\partial L}{\\partial s^i}] = \\mathbb{D}[\\frac{\\partial L}{\\partial s^j}]  $$\n",
    "\n",
    "Получается несовместная система условий \n",
    "\n",
    "$$\\forall i \\ n_i \\mathbb{D}[W^i] = 1, \\ n_{i+1} \\mathbb{D}[W^i] = 1  $$ \n",
    "\n",
    "с компромиссом (средним геометрическим) $\\mathbb{D}[W^i] = \\frac{2}{n_i + n_{i+1}}$, откуда следует, что веса можно брать, из равномерного распределения: $\\large W^i \\sim U[-\\frac{\\sqrt{6}}{\\sqrt{n_i + n_{i+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n_i + n_{i+1}}}],$ поскольку дисперсия равномерного распределения $\\mathbb{D}[U(a,b)] = \\frac{1}{12}{(b-a)}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Инициализация He\n",
    "\n",
    "> *\"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\"*, [K. He, X. Zhang et.al](https://arxiv.org/abs/1502.01852) (Microsoft Research, 2015)\n",
    "\n",
    "\n",
    "Инициализация He – специально для активации ReLU.\n",
    "\n",
    "Можно провести аналогичные рассуждения и получить:\n",
    "\n",
    "$$\\mathbb{D}[z^i] = \\mathbb{D}[x]\\prod_{k=0}^{i-1}\\frac{1}{2}n_k\\mathbb{D}[W^k] \\Rightarrow \\mathbb{D}[W^k] = \\frac{2}{n_k}$$\n",
    "$$\\mathbb{D}[\\frac{\\partial L}{\\partial s^i}]= \\mathbb{D}[\\frac{\\partial L}{\\partial s^d}]\\prod_{k=i}^{d}\\frac{1}{2}n_{k+1}\\mathbb{D}[W^k] \\Rightarrow \\mathbb{D}[W^k] = \\frac{2}{n_{k+1}}$$\n",
    "\n",
    "Можно взять только первое уравнение, и тогда \n",
    "$$\\mathbb{D}[\\frac{\\partial L}{\\partial s^i}] = \\frac{n_2}{n_d}\\mathbb{D}[\\frac{\\partial L}{\\partial s^d}],$$ \n",
    "$$W^i \\sim N(0, \\frac{2}{n_i})\\hspace{1cm} \\text{или} \\hspace{1cm} W^i \\sim N(0, \\frac{2}{n_{i+1}})$$\n",
    "На практике для сверточных сетей (в которых почти всегда используется ReLU) отношение $\\frac{n_2}{n_d}$ небольшое, и поэтому дисперсия градиентов убывает не быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Далее картинки из оригинальной статьи [K. He, et.al.](https://arxiv.org/abs/1502.01852) – сравнение инициализаций He и Xavier для 22-слойной и 30-слойной сетей. В обоих случаях ReLU-активация. В случае 30 слоев получилось даже, что сеть сошлась при инициализации He, но не сошлась с инициализацией Xavier.\n",
    "<img src='img/he_versus_xavier22.png' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='img/he_versus_xavier30.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. \"Умная\" регуляризация\n",
    "\n",
    "## Dropout\n",
    "\n",
    "> *\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"*, [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) (University of Toronto, 2014)\n",
    "\n",
    "Все вроде бы очень просто: с некоторой вероятностью $p$ зануляем активацию каждого нейрона в сети. \n",
    "\n",
    "<img src=\"./img/dropout1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Мотивация и интерпретации:\n",
    " - В мозге человека \"включены\" далеко не все связи аксонов с дендритами, иначе мозг тратил бы слишком много энергии, и мы были бы ленивцами\n",
    " - Борьба с соадаптацией – нейроны больше не могут рассчитывать на наличие соседей\n",
    " - Биология: не все гены родителей будут присутствовать у потомков\n",
    " - Усреднение астрономически большого числа моделей ($2^n$)\n",
    "<img src=\"./img/dropout2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/dropout3.png\" >\n",
    "\n",
    "```python\n",
    "class torch.nn.Dropout\n",
    "class torch.nn.Dropout2d(\n",
    "    p=0.5,         # % связей, которые нужно занулить\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Одна из вариаций: `DropConnect`. Тут с некоторой вероятностью зануляем каждую связь в сети.\n",
    "<img src=\"./img/dropconnect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "> *\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"*, [S.Ioffe, C.Szegedy](https://arxiv.org/pdf/1502.03167.pdf), (Google Inc., 2015)\n",
    "\n",
    "<img src=\"./img/batchnorm1.png\" >\n",
    "\n",
    "Мы обсуждали проблемы градиентного спуска, когда данные не нормализованы. Можно показать, что это происходит не только с входными данными, но и с выходами слоёв, даже если входные данные нормализованы (см [Andrej Karpathy cs231n](https://youtu.be/gYpoJMlgyXA?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=2221))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Решение проблемы – нормализация данных внутри сети, Batch Normalization \n",
    "\n",
    "<img src=\"./img/batchnorm2.png\" width=400>\n",
    "\n",
    "$$\n",
    "y = \\frac{x - \\mathbb{E}[x]}{ \\sqrt{\\mathbb{D}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "На практике $\\mathbb{E}[x]$ и $\\mathbb{D}[x]$ считаются по всей выборке как скользящие средние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Backprop через слой BatchNorm-слой (разбор у доски)\n",
    "\n",
    "<img src=\"./img/batchnorm_backprop.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Результаты экспериментов с ImageNet и сетью Inception ([Ioffe, Szegedy 2015](https://arxiv.org/pdf/1502.03167.pdf)):\n",
    " - BN-Baseline – Inception с BN перед каждой нелинейностью\n",
    " - BN-x5 – Inception с BN и темпом обучения, увеличенным в 5 раз (при таком темпе обучения оригинальный Inception, без BN, просто не учится)\n",
    " - BN-x30 – то же самое, но с темпом обучения, увеличенным в 30 раз\n",
    " - BN-x5-Sigmoid – то же, что BN-x5б но с $\\sigma$-нелинейностью, при которой Inception без BN не выучивал толком ничего\n",
    "\n",
    "<img src=\"./img/batchnorm_inception.png\" width=400>\n",
    "\n",
    "\n",
    "```python\n",
    "class torch.nn.BatchNorm2d(\n",
    "    num_features,   # Число фильтров, которое следует ожидать на входе\n",
    "    eps=1e-05,      # Epsilon из знаменателя\n",
    "    momentum=0.1,   # Момент, с которым идет накопление среднего и дисперсии\n",
    "    affine=True)    # учить gamma и beta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Детали:\n",
    "- Надо убрать смещения\n",
    "- Стоит использовать большие значения learning rate в начале обучения, а затем уменьшать\n",
    "- При BatchNorm другие регуляризаторы (dropout и L2) стоит уменьшить\n",
    "- Надо перемешивать обучающую выборку (чтоб не переобучаться при обучении $\\gamma$ и $\\beta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Функции активации\n",
    "\n",
    "<img src='img/all_activation_functions.png'>\n",
    "```python\n",
    "class torch.nn.Sigmoid\n",
    "```\n",
    "Чем плох сигмоид?\n",
    "<img src=\"./img/sigmoid.png\" >\n",
    "<img src=\"./img/sigmoid_deriv.png\" >\n",
    "\n",
    "```python\n",
    "class torch.nn.ReLU(inplace=False)\n",
    "```\n",
    "<img src=\"./img/relu.png\" >\n",
    "\n",
    "Активация на выходе в задаче классификации – почти всегда `Softmax`.\n",
    "```python\n",
    "class torch.nn.Softmax\n",
    "class torch.nn.Softmax2d\n",
    "```\n",
    "$$\\large {Softmax}_j(x) = \\frac{exp(x_j)}{\\sum_i exp(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Продвинутые методы оптимизации\n",
    "## SGD + Momentum\n",
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "m &\\leftarrow  \\alpha m+ (1 - \\alpha) \\nabla_{\\theta} L \\\\\n",
    "\\theta & \\leftarrow \\theta - \\eta m\n",
    "\\end{align}\n",
    "$$\n",
    "<img src=\"img/sgd_momentum.gif\" width=500>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adagrad\n",
    "В некоторых задачах хотим подстраивать темп обучения для каждого веса по отдельности. Например, в текстах для редких слов использовать больщой темп обучения, а для частых – маленький. Эта идея реализована в методе Adagrad, где для каждого веса хранится сумма квадратов градиентов по нему, и корень из этой суммы участвует в знаментале в формуле пересчета весов\n",
    "<img src='img/adagrad.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RMSProp (Root Mean Square Propagation)\n",
    "Тут в целом то же самое, что в Adagrad, только считаем экспоненциальное среднее для квадратов градиентов. \n",
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "v &\\leftarrow  \\beta v +  (1-\\beta)(\\nabla_{\\theta} L)^2 \\\\\n",
    "\\theta & \\leftarrow \\theta - \\eta \\frac{1}{\\sqrt{v} + \\epsilon} \\nabla_{\\theta} L\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "<img src=\"img/optim_comparison.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Выход из седловой точки\n",
    "<img src='img/saddle_point.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adam (Adaptive Moment Estimation) \n",
    "\n",
    "Adam = SGD + Momentum + RMSProp. Почти всегда используется именно он.\n",
    "Как и раньше, к градиентам и их квадратам применяется экспоненциальное сглаживание.\n",
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "m &\\leftarrow  \\alpha m+ (1 - \\alpha) \\nabla_{\\theta} L \\\\\n",
    "v &\\leftarrow  \\beta v +  (1-\\beta)(\\nabla_{\\theta} L)^2 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Только теперь делается еще поправки для несмещенности: $\\mathbb{E}[m_t] = \\mathbb{E}[\\nabla_{\\theta} L_t] $ и $\\mathbb{E}[v_t] = \\mathbb{E}[{(\\nabla_{\\theta} L_t)}^2]$:\n",
    "\n",
    "$$\\hat{m_t} = \\frac{m_t}{1 - \\alpha^t} \\hspace{1cm} \\hat{v_t} = \\frac{v_t}{1 - \\beta^t}$$\n",
    "\n",
    "Интуиция: вначале скорость высокая, в конце – снижается. \n",
    "Формула пересчета весов:\n",
    "$$ \\Large\n",
    "\\theta  \\leftarrow \\theta - \\eta \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} \n",
    "$$\n",
    "\n",
    "Хорошие (на практике) значения: $\\alpha$=0.9, $\\beta$=0.9, $\\epsilon = 10^{-8}$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
